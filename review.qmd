---
title: "Literature Review"
subtitle: "Summary of Papers"
author: "Susanta Deka, Kalyani Kotti (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---


### 1. Recent advances in convolutional neural networks

[link to paper](https://www.sciencedirect.com/science/article/pii/S0031320317304120)

#### What is the goal of the paper?

This paper looks at advances in the area of CNN and deep learning around 2017. Also some applications of it.

The authors look at advancements in the different categories of works related to CNN like Layer Design, Pooling Layers, Activation Function, Loss Function, Regularization, Optimization, Fast Processing, etc.

#### Why is it important?

It is important because this study showcases different techniques and different aspects of CNN to researchers interested in CNN. It provides a broad overview of all the components of CNN architecture and some deep learning techniques. The paper also looks at applications of CNN which could inspire other researchers in areas outside of machine learning to apply CNN in their domain. For example, image classification for structural engineers or Pose Estimation for Genetics Researchers or Speech Processing for Criminology Researchers.

#### How is it solved? - methods

The authors look at a wide variety of published papers and provide details about their findings and comparisons.

#### Results/limitations, if any.

This is an old paper that came out almost a decade ago in 2017. While it provides a great overview of foundational information, there have been many new developments like the use of Transformers or graph based CNNs (or graphs in general), etc.


> Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Wang, X., Wang, G., Cai, J., & Chen, T. (2018). Recent advances in convolutional neural networks. Pattern Recognition, 77, 354–377. https://doi.org/10.1016/j.patcog.2017.10.013


### 2. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks

[link to paper](https://arxiv.org/abs/1905.11946)

#### What is the goal of the paper?

The paper provides a novel network architecture and scaling method for CNN models which results in better performance. It is achieved with the use of scaling coefficients which scale the depth, width and resolution of a (prior) CNN model uniformly based on the input. The authors also provide a new family of models called the EfficientNets which are smaller and faster compared to existing CNN models.

#### Why is it important?

It is important because scaling a CNN results in higher accuracy. Moreover there are many ways of scaling a CNN, most of them on a single dimension, like the depth of a model or the resolution of the images. Previously this was a manual arbitrary task, sometimes resulting in decreased performance.

The authors state that it critical to scale a CNN in all dimensions uniformly (depth, width and resolution) for better effficiency. Based on their study, they found that a constant ratio can be used to do the scaling. They do so by figuring the constant coefficients using a grid search on the model.

#### How is it solved? - methods

Start with a baseline network and perform a small grid search to find the scaling coefficients α, β, and γ for depth, width, and resolution respectively. The compound coefficient is determined based on the hardware or compute resources available. It is an user specified quantity.
Finally both the scaling coefficients and the compound coefficient are used to calculate the optimal depth, width and resolution for a CNN network.

#### Results/limitations, if any.

This paper achieved state-of-the-art results using one of their EfficientNet models beating previous methods by being smaller and faster.

> Tan, M., & Le, Q. V. (2020). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (No. arXiv:1905.11946). arXiv. https://doi.org/10.48550/arXiv.1905.11946



### 3. Skeleton Based Action Recognition Using Multi-Scale Deep CNN

[link to paper](https://ieeexplore.ieee.org/abstract/document/8026282)

#### Goal of the Paper:

The paper explores how increasing the depth of convolutional neural networks (ConvNets) improves their accuracy in large-scale image recognition, particularly for tasks like classification and localization on ImageNet.

#### Why It’s Important:

ConvNets are key in computer vision, and this paper shows that deeper networks can boost performance, offering insights on designing more effective models for large-scale tasks and advancing the field.

#### How It’s Solved (Methods):

The authors increased the depth of ConvNets using small 3×3 convolution filters, keeping other parameters constant, and added max-pooling layers. The final architecture had multiple convolutional layers followed by fully connected layers for classification.

#### Results/Limitations:

Results: Deeper networks (16-19 layers) outperformed shallower ones, winning top spots in the 2014 ImageNet Challenge and performing well on other datasets.
Limitations: The paper focuses on depth but doesn't address other factors like data augmentation or advanced optimization, and it doesn't explore how the networks scale to even larger datasets.


### 4. Conceptual Understanding of Convolutional Neural Network- A Deep Learning Approach


[link to paper](https://www.sciencedirect.com/science/article/pii/S1877050918308019)

#### What is the goal of the paper?

The paper aims to explain Convolutional Neural Networks (CNNs), focusing on their structures, common types, and how they work. It helps researchers better understand CNNs and encourages further exploration in the field.

### Why is it Important?

CNNs are crucial for solving complex problems in areas like image and speech recognition. They automate feature extraction, making them more efficient than traditional machine learning. Understanding CNNs is essential for those working in advanced research and applications.

### How is it Solved? - Methods:

The paper explains how CNNs work by describing key layers (convolution, pooling, activation, and fully connected layers) and their roles.
It introduces popular CNN architectures like LeNet, AlexNet, and GoogleNet.
It also covers learning algorithms like Gradient Descent and ADAM, which help train CNNs.


### Results/Limitations:

Results: The paper offers a solid theoretical understanding of CNNs, showing their benefits like fewer training parameters and better generalization.
Limitations: It doesn't provide real-world results or compare CNNs to other models, and it briefly mentions issues like the "Dying ReLU" problem without exploring full solutions.


### 5. A review of convolutional neural network architectures and their optimizations


[link to paper](https://link.springer.com/article/10.1007/s10462-022-10213-5)

This article goes into details about the 6 typical architectures of CNN. CNNs typically have these five basic layers, the input layer, the convolution layer, the pooling layer, the FC layer and the output layer. The paper talks about how AlexNet used ReLu activation function instead of tanh to reduce computation complexity and resolve the vanishing gradient problem. Moreover, the use of Local Response Normalization (LRN) and pooling (down-sampling) were also helpful in improving performance as well as reducing overfitting. There's also mention of ZfNet or DeconvNet where researchers provided visualizations to understand the internal mechanism of a CNN architecture.

The article then talks about VGG Net where CNNs are split into smaller filters (or kernels) thereby increasing the depth of the network. There's also mention of GoogLeNet and its use of multiple filters of varying sizes stacked together to extract features of different sizes. The authors mention that Batch Normalization was a major contribution with the introduction of InceptionV2 model which improved backpropagation and the gradient vanishing problem.

Then there's the idea of cross-layer connection which is different from prior architectures where connections existed only between adjacent layers. ResNet implemented this cross-layer connection through shortcut connections where the input is transferred across multiple layers and then features are aggregated which improved accuracy. This is an example of widening the network which is opposite to that of deepening the network. Finally, there's info about the DenseNet which is a CNN architecture with dense connections where each layer is connected to every other layer. A bad side effect of this is that the model relearns the same features repeatedly. 

The article then looks into lightweight networks which are targeted towards mobile devices or embedded devices. Lightweight CNNs are smaller CNN architectures obtained after compression and acceleration. MobileNetV3 is a lightweight CNN that decompose the convolution process into pointwise and depthwise convolution for decreasing model size as well as compute burden.

The fifth type of CNN architecture deals with object detection. The article introduces Regional CNN (R-CNN) which achieve object detection through three main ideas which are region proposals, feature extraction and region classification. Inspired by this and based on GoogLeNet, there's a new detection method called the YOLO model, You Only Look Once which segments an input image into multiple bounding boxes and classifies the regions alongside the bounding boxes to detect objects.

Finally the last architecture is the Transformer Encoder where the article looks into Vision Transformers (ViT). Because transformers by themselves lack spatial inductive biases which leads to poor optimization, researchers have combined CNNs and ViTs to overcome it.


### 6. PooDLe: Pooled and dense self-supervised learning from naturalistic videos

[link to paper](https://arxiv.org/abs/2408.11208)

Many supervised learning methods exists which are trained on balanced datasets with human annotations. In contrast Self Supervised Learning (SSL) methods are those that tries to learn from input data without explicit supervision. PooDLe is a SSL method that learns dense visual representations from naturalistic first person videos. Natural videos are dense in information, containing multiple objects and often imbalanced, like skies dominating a nature scene or a very crowded highway scene with multiple objects.  The researchers proposed a lightweight spatial decoder module (SDM) that uses top-down decoder layers and UNet-like lateral connections to earlier encoder representations to both upsample the high level representations and resurface fine-grained details and small objects that may get lost in downsampling operations. PooDLe performs semantic information evaluation on videos. 


### 7. Integrating Convolutional Neural Networks and Reinforcement Learning for Robotics Autonomy

[link to paper](https://arxiv.org/abs/2408.11208)

This research examines the integration of Convolutional Neural Networks (CNNs) and Reinforcement Learning (RL) to enhance robot autonomy in complex environments. By utilizing CNNs for perception and RL for decision-making, the study explores advancements in robot capabilities such as real-time object recognition and adaptive decision-making. Challenges like computational complexity, sample inefficiency, and generalization across environments are addressed, with solutions including model compression, transfer learning, and data augmentation.

The paper highlights advancements in CNNs (e.g., ResNets, EfficientNets) and multi-modal sensory integration, improving robot vision and object tracking. RL techniques like Q-learning and Policy Gradients optimize decision-making and learning efficiency. Ethical considerations emphasize the need for safety and transparency, especially in sectors like healthcare and transportation.

The study concludes that combining CNNs and RL will create more intelligent, adaptive robots capable of complex tasks with minimal human input. Future work should focus on Sim-to-Real transfer, curriculum learning, and multi-modal fusion to enhance performance and robustness.


### 8. Applying Convolutional Neural Networks for IoT Image Recognition

[link to paper](https://www.researchgate.net/publication/385439626_Applying_Convolutional_Neural_Networks_for_IoT_Image_Recognition)

This article looks at how Convolutional Neural Networks (CNNs) are being used for image recognition in Internet of Things (IoT) applications, highlighting the latest progress, challenges, and possible solutions. It discusses specially designed CNN models like MobileNet, SqueezeNet, and ShuffleNet, which are optimized for IoT devices with limited resources. The paper also covers methods like model compression, pruning, quantization, and knowledge distillation to reduce the power and memory needed by these models while keeping them accurate.

A major focus is on edge computing, which processes data directly on IoT devices or nearby servers instead of sending it to the cloud. This helps to lower delays, reduce bandwidth usage, and improve security. Combining edge computing with CNNs for image recognition makes IoT systems faster and more efficient, which is especially useful in areas like self-driving cars, security cameras, and healthcare. The article concludes that more research and innovation are needed to solve the challenges of IoT and enable fast, reliable image recognition in different settings.

