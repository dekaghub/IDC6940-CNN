---
title: "Exploring the effects of Receptive Fields in CNN"
subtitle: "Optimizing CNNs through effective receptive fields"
author: "Susanta Deka, Kalyani Kotti (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references_deka.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
jupyter: python3
execute: 
  warning: false
  message: false
  cache: true
editor: 
  markdown: 
    wrap: 72
---

[Literature Review Log](review.html){target="_blank"}

Slides: [slides.html](slides.html){target="_blank"}

## Abstract

The receptive field of a Convolution Neural Network (CNN) is the
specific region/area in input data that activates the neurons i.e. the
features captured by a convolution layer. In other words, the area that a convolution layer (or filter) interacts with is the receptive field [@liCollaborativeEdgeComputing2022]. This region determines the
level of feature extraction, or learning from the input which ultimately
impacts the quality of the output alongside compute requirements and resource allocation. Different sizes of receptive fields
may be optimal for different tasks. This also means that the effective
receptive field for a given task will vary with variation in the input.
For example a magnified image vs a large zoomed out scene will yield
vastly different features for the same receptive field. In this study,
we aim to identify the optimal sizes and combination of convolution
filters for effective receptive fields as well as their compute costs. In other
words, we want to experiment different configurations of convolutional
filters and network architectures that affect the effective receptive
field using different kernel sizes and compare the number of parameters alongside the computational cost.

## Introduction

The history of Convolution Neural Networks (CNN) is a rich one and one
that spans multiple decades. CNN architecture is heavily inspired by the
biology of the visual cortex, which contains arrangements of simple and
complex cells that activate based on specific subregions of the visual
field, known as receptive fields
[@indoliaConceptualUnderstandingConvolutional2018]. CNNs have made much
of the buzz as well as many advancements in deep learning in recent
years, particularly in the domain of computer vision and image analysis.
Its application can be found in industrial settings like in fault
detection, in medical imaging for disease detection through
classification [@yamashitaConvolutionalNeuralNetworks2018], or in
consumer situations like face detection in digital cameras or object
detection in driving systems. Moreover, researchers Gu et al. point out
about the rapid growth of annotated data and the significant
improvements in the processing power of graphics processor units (GPUs)
which has empowered research on CNNs even further leading to many
state-of-the-art models and their results on various tasks.
[@guRecentAdvancesConvolutional2018]

There are numerous models and architectures of CNN. A review article by
Cong and Zhou goes into details about the 6 typical architectures of
CNN. A typical CNN consists of these five basic layers which are the input
layer, the convolution layer, the pooling layer, the Fully Connected
(FC) layer (also known as the Dense layer) and the output layer. Early advancements in CNN design are exemplified by ImageNet, more commonly known as AlexNet, which introduced the ReLu activation function instead of
tanh to reduce computation complexity and resolve the vanishing gradient
problem. Moreover, the use of Local Response Normalization (LRN) and
pooling (down-sampling) were also helpful in improving performance as
well as reducing overfitting. There's also mention of ZfNet or DeconvNet
where researchers provided visualizations to understand the internal
mechanism of a CNN architecture.

The article then talks about VGG Net where researchers used smaller 3x3 CNN
filters (or kernels) and deeper architectures with increased network depth.
There's also mention of GoogLeNet and its use of multiple filters of
varying sizes stacked together to extract features of different sizes.
The authors mention that Batch Normalization was a major contribution
with the introduction of the InceptionV2 model which improved
backpropagation and the gradient vanishing problem by stabilizing gradient flow, addressing issues related to internal covariate shift.

Then there's the idea of cross-layer connection which is different from
prior architectures where connections existed only between adjacent
layers. Residual Networks (ResNet) implemented this cross-layer connection through shortcut connections where the input is transferred across multiple layers aenabling feature aggregation across non-adjacent layers. This is an example
of widening the network which is opposite to that of deepening the
network. Finally, there's info about the DenseNet which is a CNN
architecture with dense connections where each layer is connected to
every other layer. A down side of this is that the model relearns
the same features repeatedly and there's increased computational overhead.

Lightweight architectures, such as MobileNetV3, are targeted
towards mobile devices or embedded devices. Lightweight CNNs are smaller
CNN architectures obtained after compression (reducing model size) and acceleration (increasing inference speed).
MobileNetV3 is a lightweight CNN that decompose the convolution process
into pointwise and depthwise convolution for decreasing model size as
well as compute burden.

The fifth type of CNN architecture deals with object detection. The
article introduces Regional CNN (R-CNN) which achieve object detection
through three main ideas which are region proposals, feature extraction
and region classification. Inspired by this and based on GoogLeNet,
the YOLO model, which is You Only Look Once, segments an input image into multiple bounding boxes and classifies the regions alongside the bounding boxes to detect objects.

Finally, the sixth and the last architecture is the Transformer Encoder where the
article looks into Vision Transformers (ViT). Since transformers lack spatial inductive biases which leads to poor optimization, researchers have combined CNNs and ViTs to overcome it [@congReviewConvolutionalNeural2023]. Collectively, these advancements underscore the dynamic interplay between architectural innovation and optimization in CNN evolution.

CNNs are inherently compute heavy because they contain a lot of
parameters and structural redundancy with deeper network architectures
for feature extraction. Recent advancements in lightweight deep
convolutional neural networks (DCNNs) have primarily focused on
optimizing network architecture to balance accuracy and computational
efficiency. One of the primary strategies for achieving this balance is
depthwise separable dilated convolutions, as presented in this Dual
Residual and Large Receptive Field Network (DRLN) novel architecture by
[@panDualResidualLarge2024]. Dilated convolutions introduce holes or
spaces between kernel elements, effectively increasing the receptive
field without increasing the number of parameters. ShuffleNet further
enhances this approach by incorporating group convolutions and channel
shuffling to improve information flow while maintaining
efficiency [@zhangShuffleNetExtremelyEfficient2017].

Other architectural innovations include EfficientNet's compound scaling
method, which uniformly scales depth, width, and resolution to optimize
efficiency [@tanEfficientNetRethinkingModel2020]. Furthermore, NAS
(Neural Architecture Search)-designed models have demonstrated promising
results in automating the design of lightweight networks, with
MobileNetV3 [@congReviewConvolutionalNeural2023] integrating NAS
techniques to improve upon its predecessors.

Another key direction in lightweight DCNNs is model compression, which
reduces the memory footprint and computational burden without
significantly sacrificing accuracy. Pruning techniques, such as
structured and unstructured pruning, remove redundant parameters by
eliminating less important weights or entire channels
[@iofinovaHowWellSparse2022]. Quantization, which reduces the precision
of model parameters from floating-point to lower-bit representations
(e.g., 8-bit integer), is another widely used technique that
significantly reduces memory consumption and inference latency
[@panDualResidualLarge2024]. Two widely used quantization methods are
post-training quantization (PTQ) and quantization-aware-training (QAT)
with PTQ being the more popular one since it can restore the performance
of the model without requiring the original training pipeline [@chenReviewLightweightDeep2024].


Knowledge distillation has also gained traction as an effective
compression method, wherein a large, well-trained teacher network
transfers knowledge to a smaller student network, improving the
student's performance while keeping it computationally lightweight
[@wangEdgeenhancedFeatureDistillation2022]. Additionally, low-rank
decomposition methods decompose weight matrices into smaller, efficient
representations, further enhancing compactness.

The receptive field plays a critical role in the performance of CNNs, as
it determines how much context a neuron captures. Recent studies
[@luoUnderstandingEffectiveReceptive2017] highlight that the effective
receptive field (ERF) is significantly smaller than the theoretical
receptive field, limiting the network's ability to integrate global
information efficiently. This phenomenon has implications for
lightweight CNNs, where reducing the number of parameters can
inadvertently shrink the ERF, potentially affecting recognition
accuracy.

Several strategies have been proposed to mitigate this issue in
lightweight networks. Dilated convolutions, for example, expand the
receptive field without increasing parameter count, making them
particularly useful for mobile-friendly models. Additionally, skip
connections, as seen in ResNet architectures, help maintain feature
propagation across layers, addressing the limitations imposed by a
reduced ERF.

Benchmarking lightweight CNNs requires comprehensive evaluation across
multiple datasets and hardware platforms. Common benchmark datasets such
as ImageNet, CIFAR-10, and COCO are widely used to assess classification
and detection performance
[@panDualResidualLarge2024],[@wangPooDLePooledDense2024]. However,
real-world deployment also necessitates testing on edge devices to
measure latency, power consumption, and memory efficiency.

Despite significant advancements, several challenges remain in designing
lightweight CNNs. One area of interest is the development of
hardware-aware NAS techniques that tailor models to specific device
constraints. Additionally, improving the trade-off between accuracy and
efficiency through novel loss functions and training paradigms remains
an open research problem. Finally, better understanding and optimization
of ERF in lightweight models can further enhance their effectiveness in
real-world applications.

By addressing these challenges, future research can drive further
improvements in lightweight CNNs, making them even more suitable for
deployment in resource-constrained environments such as mobile devices,
autonomous systems, and IoT applications.

## Dataset

For this project we have decided to use a dataset based on the Carla (Car Learning to Act) Driving Simulator, an open-source simulator designed for autonomous
driving research. Carla simulates driving scenarios that are applicable for training models for tasks like lane detection, object detection, and semantic segmentation.

-   Dataset Source: [Lane Detection for Carla Driving Simulator Kaggle](https://www.kaggle.com/datasets/thomasfermi/lane-detection-for-carla-driving-simulator).
-   Task: Semantic Segmentation for lane detection.
-   Image Specs: The images are RGB 1024x512 pixels.
-   Annotation: Each image has a corresponding label file, where lanes are marked with 1 & 2 (left & right lane pixels) and the background is marked with 0 (non-lane pixels).
-   Total samples: 6408 png files = 3075 train + 3075 train_label +
    129 val + 129 val_label images

## Dataset Visualization

The dataset contains over 6000 images of road scenes and label images
for training and validation.

Example of one of the images.

![](https://imgur.com/sAVhGgC)

The label images are grayscale pngs with 3 pixel values, which are
0,1,2.

![](https://imgur.com/2A3k7z3)

The following image is the label image visualized for human eyes with
visually distinct color values. Here we can see the lane lines which
serve as the label for training.

![](https://imgur.com/LAKKKlk)

### Exploring summary statistics of the dataset images

Since the images contain all three RGB channels, we perform some
statistics to understand the distribution of each individual RGB channel
as well as the RGB colors themselves.

```{python}
#| cache: true
import numpy as np
from PIL import Image
from skimage import color as skcolor
from collections import Counter
import matplotlib.pyplot as plt

def analyze_image(image_path):
    img = Image.open(image_path)
    img_array = np.array(img)

    rgb_means = np.mean(img_array, axis=(0, 1))
    rgb_medians = np.median(img_array, axis=(0, 1))
    rgb_mins = np.min(img_array, axis=(0, 1))
    rgb_maxs = np.max(img_array, axis=(0, 1))
    rgb_stds = np.std(img_array, axis=(0, 1))

    # Color distribution
    rgb_hist = [np.histogram(img_array[:,:,i], bins=256, range=(0,256))[0] for i in range(3)]

    # Dominant colors (top 5)
    pixels = img_array.reshape(-1, 3)
    counts = Counter(map(tuple, pixels))
    dominant_colors = counts.most_common(5)

    # Luminosity - the array of number is based on how the human eye perceives brightness in the RGB channels
    luminosity = np.dot(rgb_means, [0.299, 0.587, 0.114])

    print(f"RGB Means: R={rgb_means[0]:.2f}, G={rgb_means[1]:.2f}, B={rgb_means[2]:.2f}")
    print(f"RGB Medians: R={rgb_medians[0]:.2f}, G={rgb_medians[1]:.2f}, B={rgb_medians[2]:.2f}")
    print(f"RGB Mins: R={rgb_mins[0]}, G={rgb_mins[1]}, B={rgb_mins[2]}")
    print(f"RGB Maxs: R={rgb_maxs[0]}, G={rgb_maxs[1]}, B={rgb_maxs[2]}")
    print(f"RGB Standard Deviations: R={rgb_stds[0]:.2f}, G={rgb_stds[1]:.2f}, B={rgb_stds[2]:.2f}")
    print(f"Luminosity: {luminosity:.2f}")


    print("\n\nTop 5 Dominant Colors (RGB, count):")
    for rgb_vals, count in dominant_colors:
        print(f"  {rgb_vals[0]:4d} {rgb_vals[1]:4d} {rgb_vals[2]:4d} : {count}")

    # Plot histograms
    fig, axs = plt.subplots(3, 1, figsize=(10, 10))
    colors = ['red', 'green', 'blue']
    for i, (hist, color) in enumerate(zip(rgb_hist, colors)):
        axs[i].bar(range(256), hist, color=color, alpha=0.7)
        axs[i].set_title(f'{color.capitalize()} Channel Histogram')
    plt.tight_layout()
    plt.show()

# Usage
analyze_image("C:/Users/deka_/Desktop/Testing/Datasets/carla-lane/train/Town04_Clear_Noon_09_09_2020_14_57_22_frame_569.png")
```

Summary statistics of a sample of the training dataset. In this case
we're taking 10% of the dataset as our sample.

```{python}
#| cache: true

import os

base_dir = "C:/Users/deka_/Desktop/Testing/Datasets/carla-lane/train"
all_files = os.listdir(base_dir)

all_R = np.empty((0,), dtype=np.uint8)
all_G = np.empty((0,), dtype=np.uint8)
all_B = np.empty((0,), dtype=np.uint8)
all_dominants = Counter()

sample_len = int(len(all_files) * .1)

for file in all_files[:sample_len]:
    img = Image.open(f"{base_dir}/{file}")
    img_array = np.array(img)

    pixels = img_array.reshape(-1, 3)

    all_R = np.concatenate((all_R, pixels[:,0]))
    all_G = np.concatenate((all_G, pixels[:,1]))
    all_B = np.concatenate((all_B, pixels[:,2]))

    counts = Counter(map(tuple, pixels))
    dominant_colors = counts.most_common(5)
    dominant_dict = dict(dominant_colors)
    all_dominants.update(dominant_dict)
```

> Summary statistics of the RGB channels of the sample.

```{python}
print(f"RGB Means:    R = {round(np.mean(all_R)):>3}, G = {round(np.mean(all_G)):>3}, B = {round(np.mean(all_B)):>3}")
print(f"RGB Medians:  R = {round(np.median(all_R)):>3}, G = {round(np.median(all_G)):>3}, B = {round(np.median(all_B)):>3}")
print(f"RGB Mins:     R = {round(np.min(all_R)):>3}, G = {round(np.min(all_G)):>3}, B = {round(np.min(all_B)):>3}")
print(f"RGB Maxs:     R = {round(np.max(all_R)):>3}, G = {round(np.max(all_G)):>3}, B = {round(np.max(all_B)):>3}")
print(f"RGB Std Devs: R = {round(np.std(all_R)):>3}, G = {round(np.std(all_G)):>3}, B = {round(np.std(all_B)):>3}")
```

> The top 5 RGB colors in the sample and their count.

```{python}
for rgb_vals, count in all_dominants.most_common(5):
    print(f"  {rgb_vals[0]:4d} {rgb_vals[1]:4d} {rgb_vals[2]:4d} : {count}")
```

```{python}
#| cache: true
rgb_hist = [
    np.histogram(all_R, bins=256, range=(0,256))[0],
    np.histogram(all_G, bins=256, range=(0,256))[0],
    np.histogram(all_B, bins=256, range=(0,256))[0]
]

fig, axs = plt.subplots(3, 1, figsize=(10, 10))
colors = ['red', 'green', 'blue']
for i, (hist, color) in enumerate(zip(rgb_hist, colors)):
    axs[i].bar(range(256), hist, color=color, alpha=0.7)
    axs[i].set_title(f'{color.capitalize()} Channel Histogram')
    axs[i].set_xlim(0, 255)  # Set consistent x-axis limits
    axs[i].set_xlabel('Pixel Value')
    axs[i].set_ylabel('Frequency')

fig.suptitle('RGB Channel Distributions Across Sample Images', fontsize=16)
plt.tight_layout()
plt.show()
```

## Methods

The main objective of this project is to compare 3×3, 5×5, and 7×7 convolution filters within two architectures: a basic CNN-based encoder-decoder and the UNet model. We aim to evaluate how filter size affects segmentation accuracy, training and evaluation time, and overall computational efficiency. The main machine learning task is semantic segmentation, specifically detecting lanes in a road. Unlike image classification where a label is assigned to an entire image, semantic segmentation goes a step further by classifying each pixel in the image to some class label. Semantic segmentation requires a model to output the final result in the same dimension as the input image. This means that information from the input needs to be carried over in all the layers. The final result is a label map or mask where different pixel values represent different class labels.

The following steps outline the experimental setup:

**1. Network Architecture**

> 1.a CNN Encoder-Decoder

![](./imgs/model_arch_cnn-encoder-decoder.png)

Encoder - The encoder comprises five sequential blocks, each containing:

* Strided Convolution Layers:

    * Kernel size: {3×3, 5×5, 7×7} - 3 variants of the model

    * Stride 2 with padding {1, 2, 3} for the 3 variants

    * Channel expansion: 3→32→64→128→256→512

* Feature Normalization:

    * Batch normalization (BN) layer after each convolution

* Non-linear Activation:

    * ReLU activation for feature transformation

The encoder downsamples the spatial dimensions by $2^5$ (32x downscaling).

Decoder - The decoder mirrors the encoder structure using transposed convolutions:

* Upsampling Layers:

    * Kernel-matched transposed convolutions (stride 2)

    * Output padding (1) to recover original resolution

    * Channel reduction: 512→256→128→64→32→3

* Normalization & Activation:

    * BN + ReLU combinations in all layers except final output

* Reconstruction Layer:
    
    * Final transposed convolution produces 3-channel output matching input dimensions
    

> 1.b U-Net

![](./imgs/model_arch_unet.png)

Encoder (Contracting Path) - The encoder consists of four sequential blocks, each containing:

* Double Convolutional Layers:

    * Each block applies two consecutive convolutional layers:

        * Kernel size: {3×3, 5×5, 7×7} - 3 variants of the model

        * Channel expansion: 3→64→128→256→512

        * Feature Normalization: Batch normalization (BN) after each convolution

        * Non-linear Activation: ReLU activation after each BN

    * Downsampling:

        * Max pooling (2×2, stride 2) after each block halves the spatial resolution

The encoder downsamples the spatial dimensions by a factor of $2^4$ (16× downscaling).

* Bottleneck

    * Two convolutional layers (with BN and ReLU) at the bottleneck

    * Channels: 512→1024

Decoder (Expanding Path) - The decoder mirrors the encoder structure and restores the original resolution using:

* Upsampling Layers:

    * Transposed convolutions (kernel size 2×2, stride 2) double the spatial dimensions at each step

    * Channel reduction: 1024→512→256→128→64

* Skip Connections:

    * At each upsampling stage, the upsampled feature map is concatenated with the corresponding encoder feature map (skip connection) to preserve spatial context

    * If spatial dimensions do not match (due to rounding), the upsampled map is resized to match the encoder feature

* Double Convolutional Layers:

    * After concatenation, two convolutional layers (with BN and ReLU) refine the combined features

Output Layer

* Reconstruction Layer:

    * A final 1×1 convolution layer with 64 in channels maps the features to the desired number of output channels (3 in our case)

    * Produces an output with the same spatial dimensions as the input

**2. Training Configuration**

For the model training, the following setup will be used: Loss Function:
The cross-entropy loss function will be used for training. This is a
standard loss function for semantic segmentation tasks that penalizes
the difference between predicted and ground truth segmentation masks.
The formula for the cross-entropy loss for binary classification (e.g.,
predicting whether each pixel belongs to a lane or not) is:

$\text{Loss} = -\left[ y \log(p) + (1 - y) \log(1 - p) \right]$

Where:

-   y is the true label (0 or 1) of a given pixel (1 if it's a lane, 0
    if it's not).
-   p is the predicted probability that the pixel belongs to class 1
    (lane).
-   log is the natural logarithm. For multi-class classification (such
    as when there are more than two classes, like lane types or road
    segments), the formula generalizes as follows:

$\text{Loss} = -\sum_{c=1}^C Y_c \log(p_c)$

Where: \* C is the number of classes (e.g., "lane", "road",
"background"). \* $Y_c$ is the true label for class ccc (1 if the pixel
belongs to class ccc, 0 otherwise). \* $P_c$ is the predicted
probability for class ccc for a given pixel. \* The sum is taken over
all classes. \* Optimizer: The Adam optimizer will be used with an
initial learning rate of 0.001. Learning rate scheduling will be
implemented to adjust the learning rate during training, ensuring better
convergence. The Adam optimizer computes updates using the following
steps:

1.  Gradient Calculation: Let θt be the parameters at time step t, and
    gt be the gradient of the loss function at time step t with respect
    to the parameters.

2.  Compute the Moving Averages:

-   The first moment estimate mt is the exponentially weighted moving
    average of the gradients:
    $m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$
-   The second moment estimate vt is the exponentially weighted moving
    average of the squared gradients:
    $v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$
-   Here, β1 and β2 are hyperparameters that control the decay rates of
    the moving averages for the first and second moments, respectively.
    Commonly used values are:
-   $\beta_1=0.9$
-   $\beta_2=0.999$

3.  Bias Correction: Since the moving averages are initialized to 0,
    they are biased toward 0 during the early time steps. To correct
    this, bias-corrected estimates (\hat{m}\_t) and (\hat{v}\_t) are
    computed as: \[ \hat{m}\_t = \frac{m_t}{1 - \beta_1^t} \] \[
    \hat{v}\_t = \frac{v_t}{1 - \beta_2^t} \] The bias correction terms
    help to ensure that the moment estimates are unbiased, especially in
    the early stages of training.

4.  Parameter Update: Finally, the parameters are updated using the
    following rule:

$\theta_{t+1} = \theta_t - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$

Where:

-   α is the learning rate (often set to 0.001),

-   $\epsilon$ is a small constant (e.g., $(10^{-7})$) to avoid division
    by zero.

-   Batch Size: A batch size of 16 will be used to balance between
    computational efficiency and memory constraints.

-   Training and Validation Split: 80% of the dataset will be used for
    training, and the remaining 20% will be used for validation to
    monitor performance and prevent overfitting.

**4. Receptive Field Analysis**

The receptive field will be manipulated and analyzed through the
following strategies:

-   Varying Kernel Sizes: Models will be tested using convolutional
    kernels of 3x3, 5x5, and 7x7 sizes to investigate the trade-off
    between local feature capture and global context. The goal is to
    determine how kernel size affects lane detection accuracy,
    especially in complex road structures.
-   Increasing Network Depth: The effect of deeper architectures on
    receptive field size will be explored. Deeper networks will
    naturally increase the receptive field, which might help in
    detecting lanes across long distances or in intricate road layouts.
    We will compare shallow and deep architectures to assess the
    benefits.
-   Dilated Convolutions: Dilated convolutions will be incorporated into
    the network to expand the receptive field without down sampling.
    This allows the model to capture larger areas of the image while
    retaining spatial resolution for finer lane features.
-   Pooling Layers: Different configurations of pooling layers (e.g.,
    max pooling vs. average pooling) will be evaluated to observe how
    they influence the receptive field. Pooling is known to increase the
    receptive field, but too much pooling early in the network might
    cause the loss of fine lane details.

**5. Hyperparameter Tuning**

The following hyperparameters will be tuned to optimize model
performance:

-   Learning Rate: Experimentation with different learning rates to find
    the optimal value for faster convergence.
-   Number of Epochs: A sufficient number of epochs will be used (e.g.,
    50-100 epochs) to allow the model to converge. Early stopping will
    be implemented to prevent overfitting.
-   Batch Size: We will test different batch sizes to determine the best
    trade-off between memory usage and training stability.

**6. Evaluation Metrics**

The following evaluation metrics will be used to assess the performance
of the models:

-   Pixel Accuracy: Measures the percentage of pixels that are correctly
    classified as part of the lane or background.
-   Mean Intersection over Union (mIoU): Calculates the overlap between
    predicted and ground truth lane masks, providing a measure of
    segmentation accuracy.
-   Dice Similarity Coefficient (DSC): This metric evaluates the
    similarity between the predicted and true lane masks, which is
    particularly important for imbalanced datasets.

**7. Model Evaluation**

Once trained, the model will be evaluated on the test set using the
aforementioned metrics. The impact of receptive field adjustments
(kernel sizes, network depth, dilated convolutions, pooling) on the
model's performance will be carefully analyzed, particularly with
respect to its ability to detect lane boundaries in different road
conditions. Expected Outcomes We hypothesize that:

-   Larger receptive fields (via larger kernels, deeper networks, and
    dilated convolutions) will improve the model's ability to detect
    long-range lane structures, especially on curved or complex roads.
-   Smaller receptive fields may be better for focusing on localized
    features, such as lane markings, but may struggle with more complex
    road geometries.
-   The encoder-decoder architecture will improve the segmentation
    accuracy by allowing the model to retain fine details while
    capturing broader contextual information. The results will guide us
    in identifying the most effective network configuration for optimal
    lane detection performance in autonomous driving applications.

# Arrange Later

Since we are working on a task that has inherent imbalance because the
target accounts for fraction of the total area. Therefore dice loss.
[@katoAdaptiveTvMFDice2024]

## Analysis and Results

# References
