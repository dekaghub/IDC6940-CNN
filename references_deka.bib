
@article{yamashitaConvolutionalNeuralNetworks2018,
	title = {Convolutional neural networks: an overview and application in radiology},
	volume = {9},
	issn = {1869-4101},
	shorttitle = {Convolutional neural networks},
	url = {https://insightsimaging.springeropen.com/articles/10.1007/s13244-018-0639-9},
	doi = {10.1007/s13244-018-0639-9},
	language = {en},
	number = {4},
	urldate = {2025-01-21},
	journal = {Insights into Imaging},
	author = {Yamashita, Rikiya and Nishio, Mizuho and Do, Richard Kinh Gian and Togashi, Kaori},
	month = aug,
	year = {2018},
	pages = {611--629},
	file = {Full Text:C\:\\Users\\deka_\\Zotero\\storage\\R47LZKYW\\Yamashita et al. - 2018 - Convolutional neural networks an overview and application in radiology.pdf:application/pdf},
}

@article{guRecentAdvancesConvolutional2018,
	title = {Recent advances in convolutional neural networks},
	volume = {77},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320317304120},
	doi = {10.1016/j.patcog.2017.10.013},
	language = {en},
	urldate = {2025-01-28},
	journal = {Pattern Recognition},
	author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
	month = may,
	year = {2018},
	pages = {354--377},
	file = {Submitted Version:C\:\\Users\\deka_\\Zotero\\storage\\QCH33EHW\\Gu et al. - 2018 - Recent advances in convolutional neural networks.pdf:application/pdf},
}

@misc{tanEfficientNetRethinkingModel2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	doi = {10.48550/arXiv.1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv:1905.11946 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\deka_\\Zotero\\storage\\IY2FJCAS\\Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf:application/pdf;Snapshot:C\:\\Users\\deka_\\Zotero\\storage\\36LCTW49\\1905.html:text/html},
}

@article{congReviewConvolutionalNeural2023,
	title = {A review of convolutional neural network architectures and their optimizations},
	volume = {56},
	issn = {0269-2821, 1573-7462},
	url = {https://link.springer.com/10.1007/s10462-022-10213-5},
	doi = {10.1007/s10462-022-10213-5},
	language = {en},
	number = {3},
	urldate = {2025-02-03},
	journal = {Artificial Intelligence Review},
	author = {Cong, Shuang and Zhou, Yang},
	month = mar,
	year = {2023},
	pages = {1905--1969},
	file = {PDF:C\:\\Users\\deka_\\Zotero\\storage\\CYIT5VV7\\Cong and Zhou - 2023 - A review of convolutional neural network architectures and their optimizations.pdf:application/pdf},
}

@article{chenReviewLightweightDeep2024,
	title = {Review of {Lightweight} {Deep} {Convolutional} {Neural} {Networks}},
	volume = {31},
	issn = {1134-3060, 1886-1784},
	url = {https://link.springer.com/10.1007/s11831-023-10032-z},
	doi = {10.1007/s11831-023-10032-z},
	language = {en},
	number = {4},
	urldate = {2025-02-03},
	journal = {Archives of Computational Methods in Engineering},
	author = {Chen, Fanghui and Li, Shouliang and Han, Jiale and Ren, Fengyuan and Yang, Zhen},
	month = may,
	year = {2024},
	pages = {1915--1937},
	file = {PDF:C\:\\Users\\deka_\\Zotero\\storage\\KU6EFVRU\\Chen et al. - 2024 - Review of Lightweight Deep Convolutional Neural Networks.pdf:application/pdf},
}

@misc{wangPooDLePooledDense2024,
	title = {{PooDLe}: {Pooled} and dense self-supervised learning from naturalistic videos},
	shorttitle = {{PooDLe}},
	url = {http://arxiv.org/abs/2408.11208},
	doi = {10.48550/arXiv.2408.11208},
	abstract = {Self-supervised learning has driven significant progress in learning from single-subject, iconic images. However, there are still unanswered questions about the use of minimally-curated, naturalistic video data, which contain dense scenes with many independent objects, imbalanced class distributions, and varying object sizes. In this paper, we propose a novel approach that combines an invariance-based SSL objective on pooled representations with a dense SSL objective that enforces equivariance to optical flow warping. Our findings indicate that a unified objective applied at multiple feature scales is essential for learning effective image representations from high-resolution, naturalistic videos. We validate our approach on the BDD100K driving video dataset and the Walking Tours first-person video dataset, demonstrating its ability to capture spatial understanding from a dense objective and semantic understanding via a pooled representation objective.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Wang, Alex N. and Hoang, Christopher and Xiong, Yuwen and LeCun, Yann and Ren, Mengye},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11208 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PooDLe - Pooled and dense self-supervised learning from naturalistic videos:C\:\\Users\\deka_\\Zotero\\storage\\PIYJKMAV\\PooDLe - Pooled and dense self-supervised learning from naturalistic videos.pdf:application/pdf;Preprint PDF:C\:\\Users\\deka_\\Zotero\\storage\\2MXREWCD\\Wang et al. - 2024 - PooDLe Pooled and dense self-supervised learning from naturalistic videos.pdf:application/pdf;Snapshot:C\:\\Users\\deka_\\Zotero\\storage\\YS3YHJI9\\2408.html:text/html},
}

@article{panDualResidualLarge2024,
	title = {Dual residual and large receptive field network for lightweight image super-resolution},
	volume = {600},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231224009299},
	doi = {10.1016/j.neucom.2024.128158},
	language = {en},
	urldate = {2025-02-10},
	journal = {Neurocomputing},
	author = {Pan, Lulu and Li, Guo and Xu, Ke and Lv, Yanheng and Zhang, Wenbo and Li, Lingxiao and Lei, Le},
	month = oct,
	year = {2024},
	pages = {128158},
}

@misc{luoUnderstandingEffectiveReceptive2017,
	title = {Understanding the {Effective} {Receptive} {Field} in {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1701.04128},
	doi = {10.48550/arXiv.1701.04128},
	abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
	month = jan,
	year = {2017},
	note = {arXiv:1701.04128 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\deka_\\Zotero\\storage\\YRMFGQ64\\Luo et al. - 2017 - Understanding the Effective Receptive Field in Deep Convolutional Neural Networks.pdf:application/pdf;Snapshot:C\:\\Users\\deka_\\Zotero\\storage\\3RPE89D4\\1701.html:text/html},
}

@article{indoliaConceptualUnderstandingConvolutional2018,
	title = {Conceptual {Understanding} of {Convolutional} {Neural} {Network}- {A} {Deep} {Learning} {Approach}},
	volume = {132},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918308019},
	doi = {10.1016/j.procs.2018.05.069},
	language = {en},
	urldate = {2025-02-18},
	journal = {Procedia Computer Science},
	author = {Indolia, Sakshi and Goswami, Anil Kumar and Mishra, S.P. and Asopa, Pooja},
	year = {2018},
	pages = {679--688},
}

@misc{zhangShuffleNetExtremelyEfficient2017,
	title = {{ShuffleNet}: {An} {Extremely} {Efficient} {Convolutional} {Neural} {Network} for {Mobile} {Devices}},
	shorttitle = {{ShuffleNet}},
	url = {http://arxiv.org/abs/1707.01083},
	doi = {10.48550/arXiv.1707.01083},
	abstract = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8\%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves {\textasciitilde}13x actual speedup over AlexNet while maintaining comparable accuracy.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
	month = dec,
	year = {2017},
	note = {arXiv:1707.01083 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\deka_\\Zotero\\storage\\3M8HW5QJ\\Zhang et al. - 2017 - ShuffleNet An Extremely Efficient Convolutional Neural Network for Mobile Devices.pdf:application/pdf;Snapshot:C\:\\Users\\deka_\\Zotero\\storage\\7I9DLH2M\\1707.html:text/html},
}

@inproceedings{iofinovaHowWellSparse2022,
	address = {New Orleans, LA, USA},
	title = {How {Well} {Do} {Sparse} {ImageNet} {Models} {Transfer}?},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-6946-3},
	url = {https://ieeexplore.ieee.org/document/9878988/},
	doi = {10.1109/CVPR52688.2022.01195},
	urldate = {2025-03-03},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Iofinova, Eugenia and Peste, Alexandra and Kurtz, Mark and Alistarh, Dan},
	month = jun,
	year = {2022},
	pages = {12256--12266},
	file = {Submitted Version:C\:\\Users\\deka_\\Zotero\\storage\\4K8C78AQ\\Iofinova et al. - 2022 - How Well Do Sparse ImageNet Models Transfer.pdf:application/pdf},
}

@inproceedings{wangEdgeenhancedFeatureDistillation2022,
	address = {New Orleans, LA, USA},
	title = {Edge-enhanced {Feature} {Distillation} {Network} for {Efficient} {Super}-{Resolution}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-8739-9},
	url = {https://ieeexplore.ieee.org/document/9857361/},
	doi = {10.1109/CVPRW56347.2022.00093},
	urldate = {2025-03-03},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Wang, Yan},
	month = jun,
	year = {2022},
	pages = {776--784},
	file = {Submitted Version:C\:\\Users\\deka_\\Zotero\\storage\\9ASW2SPP\\Wang - 2022 - Edge-enhanced Feature Distillation Network for Efficient Super-Resolution.pdf:application/pdf},
}
